# networkLLM â€” Intent-Aware Intrusion Detection with LLMs

**What this does**  
- Reproducible scaffold to infer **packet/flow intent** (benign vs malicious) using LLMs grounded by protocol semantics.
- Includes a **LightGBM baseline**, **field-aware LLM classifier**, RFC **RAG** module, and evaluation harness (PR-AUC/F1).

**Why**  
- Traditional IDS focus on signatures. This project surfaces **intended behaviour** (handshake, probe, exploit, exfiltration) and provides **explanations** grounded in RFCs/specs.

## Directory layout
```
networkLLM_full/
â”œâ”€ src/networkllm/
â”‚  â”œâ”€ config.py                 # Sanity Config â€” single source of truth
â”‚  â”œâ”€ preprocess/
â”‚  â”‚  â”œâ”€ zeek.py                # Zeek/Wireshark JSON loaders
â”‚  â”‚  â”œâ”€ tokenise.py            # Field-aware tokenisation â†’ LLM text
â”‚  â”‚  â””â”€ preprocessor.py        # Derive preprocessor + feature lists
â”‚  â”œâ”€ representations/
â”‚  â”‚  â””â”€ field_text.py          # Bytes-aware helpers (entropy, length)
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ baseline_lgbm.py       # Baseline LightGBM (tabular flows)
â”‚  â”‚  â””â”€ llm_classifier.py      # DistilBERT classifier over tokens
â”‚  â”œâ”€ eval/
â”‚  â”‚  â””â”€ metrics.py             # Metrics passthrough
â”‚  â””â”€ rag/
â”‚     â””â”€ spec_index.py          # Minimal FAISS index for RFC RAG
â”œâ”€ examples/
â”‚  â”œâ”€ run_baseline.py           # Train baseline and print report
â”‚  â””â”€ run_llm_intent.py         # Train LLM classifier and print report
â”œâ”€ data/                        # Place CSVs/JSON here
â”œâ”€ staging/                     # Manifests and intermediate artefacts
â”œâ”€ out/                         # Models, reports, charts
â”œâ”€ requirements.txt
â””â”€ LICENSE
```

## Quickstart
1. Create a Python 3.12+ environment, `pip install -r requirements.txt`.
2. Place a labelled CSV under `data/` (e.g., CIC-IDS2017/UNSW-NB15/MAWIFlow export).
3. Edit `src/networkllm/config.py` only (single source of truth) to set `DATA_PATH`, `TARGET_COL`, `stage_root`.
4. Baseline: `python -m networkllm.models.baseline_lgbm`
5. LLM intent: `python examples/run_llm_intent.py`

**Conventions (Imperial naming):** `X_train/y_train`, `X_val/y_val`, `X_test/y_test`; `feature_names`, `cat_cols`, `num_cols`; `baseline_model`, `bayes`; `RANDOM_STATE`, `DATA_PATH`, `TARGET_COL`, `stage_root`.

---

## Other related researchs

| Paper / Project                                      | Dataset(s) Used                               | Method                                             | Key Results                                                      | Implications for **networkLLM**                                                                                       |
| ---------------------------------------------------- | --------------------------------------------- | -------------------------------------------------- | ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| **LLM-Guided Protocol Fuzzing (ChatAFL, NDSS 2024)** | Real protocol parsers (e.g., FTP, SMTP, HTTP) | LLM extracts grammar + generates seeds for fuzzing | Higher **coverage**, new states found, escaped coverage plateaus | Use LLMs not just for detection, but for **hard-negative generation** (augment training with fuzzed malicious seeds). |
| **PROSPER (HotNets 2023)**                           | RFCs (TLS, DNS, HTTP)                         | LLM extracts FSMs from specs                       | Accurate state machines built from RFC text                      | Ground intent detection in **RFC-derived state transitions**; enables â€œviolates RFC section Xâ€ explanations.          |
| **Packet Field Tree (ACM 2024)**                     | Open traces + crafted protocols               | Hybrid learning of field hierarchies               | Better field extraction accuracy vs heuristics                   | Adopt **field-aware tokenisation** (e.g., `[tcp.dst_port=445]`) for LLM input; avoids raw hex ambiguity.              |
| **LLMcap (2024)**                                    | PCAP traces (network failures/anomalies)      | Self-supervised LLM anomaly detection              | Outperformed statistical anomaly baselines                       | LLMs can **detect unseen threats** without labels; useful for zero-day scenarios.                                     |
| **TrafficLLM (2025)**                                | CIC-IDS2017, UNSW-NB15 + custom tokenisation  | Domain-specific tokenisation + fine-tuning         | Higher classification accuracy, more stable on imbalance         | Validates **tokenisation strategy** in `networkLLM`: combine field-tokens with byte-level features.                   |
| **Arkko (ANRW 2024)**                                | Raw socket/packet traces                      | Direct LLM application to low-level traffic        | Struggled on raw bytes, better on diagnostic narratives          | Reinforces: preprocess packets â†’ **structured text** before LLM; avoid raw binary input.                              |
| **LLMPot (ICS Honeypot, 2024)**                      | ICS traffic (Modbus, DNP3)                    | LLM-generated honeypot responses                   | More realistic, intent-driven responses than rule-based          | LLMs capture **intent semantics**, useful for both IDS explanations and deception tools.                              |

---

## ğŸ”‘ Key Take-aways for **networkLLM**

* **Must:** Build on **field-aware tokens** (Packet Field Tree + TrafficLLM).
* **Should:** Add **RFC/RAG grounding** (PROSPER) for explainable alerts.
* **May:** Use **LLM-guided fuzzing (ChatAFL)** to mine hard negatives and augment training.
* **Can:** Leverage **LLMcap-style anomaly detection** for unlabeled traffic (zero-day hunting).
* **Avoid:** Feeding raw packet bytes (Arkko) â†’ always normalise first.
* **Explore:** Extending intent detection into **active deception** (LLMPot-style honeypots).

---



